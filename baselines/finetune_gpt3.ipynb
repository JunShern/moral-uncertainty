{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Q4RL-Rd-kSg"
      },
      "source": [
        "Based on https://github.com/openai/openai-cookbook/blob/main/examples/Fine-tuned_classification.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TRlNSQJoheaf",
        "outputId": "c80c6fde-4b95-42d7-91d7-ae26554afb59"
      },
      "outputs": [],
      "source": [
        "!pip install gdown\n",
        "!pip install --upgrade openai\n",
        "!pip install transformers\n",
        "!pip install wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gVo_vYy9h769",
        "outputId": "57e0355c-81f5-4295-8f8b-f3e165c4c445"
      },
      "outputs": [],
      "source": [
        "!wandb login\n",
        "%env OPENAI_API_KEY=\"MY_API_KEY\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKIPYlLS9wia"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i7ajSUg96xO-",
        "outputId": "51875806-1a82-4322-d373-b86bad0d7ba1"
      },
      "outputs": [],
      "source": [
        "# Download train and test datasets\n",
        "!gdown 1je2h8QdkzC2hhBl-Mqy0lPYSKs5-Buwp\n",
        "!gdown 1jFXMCf0QM-QdBJnExDis8sh_BLuiEPjD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "0_7jIeaa8shN",
        "outputId": "3b7aaa8d-4cdf-4b25-e0e7-e066dd3d0efe"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Our choices for labels and separator based on the default recommendations from OpenAI; there might be better choices\n",
        "pos_label = \" bad\"\n",
        "neg_label = \" good\"\n",
        "separator = \"\\n\\n###\\n\\n\"\n",
        "\n",
        "\n",
        "df_train = pd.read_csv(\"train.csv\")\n",
        "df_train.loc[df_train['label'] == 1, 'label'] = pos_label\n",
        "df_train.loc[df_train['label'] == 0, 'label'] = neg_label\n",
        "df_train.columns = ['prompt', 'completion']\n",
        "\n",
        "# Add custom separator at the end of the prompts\n",
        "for i, input in enumerate(df_train['prompt']):\n",
        "    df_train['prompt'][i] = input + separator\n",
        "\n",
        "display(df_train)\n",
        "df_train.to_json(\"moraluncertainty_train.jsonl\", orient='records', lines=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KIdm3IMksA6X",
        "outputId": "4ff7f5ae-4d20-4c76-80b4-65293800d116"
      },
      "outputs": [],
      "source": [
        "!rm moraluncertainty_train_prepared_*.jsonl\n",
        "!openai tools fine_tunes.prepare_data -f moraluncertainty_train.jsonl -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zX6kFpaxjNpK",
        "outputId": "aae850cb-d6a5-4919-e79f-771a6381d79c"
      },
      "outputs": [],
      "source": [
        "!tail moraluncertainty_train_prepared_train.jsonl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L7PJCJnis6HX"
      },
      "source": [
        "# Fine-tune"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VURfq43js6SE",
        "outputId": "5923f386-c292-43ab-b152-e08413fe4a67"
      },
      "outputs": [],
      "source": [
        "!openai api fine_tunes.create -t \"moraluncertainty_train_prepared_train.jsonl\" -v \"moraluncertainty_train_prepared_valid.jsonl\" --compute_classification_metrics --classification_positive_class \" bad\" -m davinci --n_epochs 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set IDs from the above output\n",
        "%env FT_RUN_ID=MY_FT_RUN_ID\n",
        "%env FT_MODEL_ID=MY_FT_MODEL_ID"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qd0aSGs1BCDm",
        "outputId": "44118f64-1ac4-47b8-e29c-9b6d324517f0"
      },
      "outputs": [],
      "source": [
        "# If you need to resume monitoring\n",
        "!openai api fine_tunes.follow -i $FT_RUN_ID"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AwttY2dos6ck",
        "outputId": "068526ad-290a-4a9b-8be4-909dc98e735e"
      },
      "outputs": [],
      "source": [
        "!openai wandb sync --project moral-uncertainty-gpt-3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 908
        },
        "id": "zhsJxR0KtkeB",
        "outputId": "4bbcb380-1f88-4bd1-c147-57cfc2d98374"
      },
      "outputs": [],
      "source": [
        "# Check train and validation results\n",
        "!openai api fine_tunes.results -i $FT_RUN_ID > result.csv\n",
        "\n",
        "results = pd.read_csv('result.csv')\n",
        "print(results[results['classification/accuracy'].notnull()])\n",
        "results[results['classification/accuracy'].notnull()]['classification/accuracy'].plot()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZV8KPw_Nt2Oa"
      },
      "source": [
        "# Test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "vHnEMzlKUEg5",
        "outputId": "1ba6a1b0-bf7c-4c99-80f3-837cac54c21b"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df_test = pd.read_csv(\"test.csv\")\n",
        "df_test.columns = ['prompt']\n",
        "assert len(df_test) == 2771\n",
        "df_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QdllHovNuCTR",
        "outputId": "732f9058-5ecf-4853-cb1f-b047f8bcd70f"
      },
      "outputs": [],
      "source": [
        "# Run inference for all test examples\n",
        "\n",
        "import os\n",
        "import openai\n",
        "from tqdm import tqdm\n",
        "from transformers import GPT2TokenizerFast\n",
        "\n",
        "# The model that we want to run inference on\n",
        "ft_model = os.environ[\"FT_RUN_ID\"]\n",
        "\n",
        "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
        "# Set logit_bias to 100 for our two output classes to ensure the model only predicts these two options\n",
        "tokenized_labels = [tokenizer.encode(label)[0] for label in [pos_label, neg_label]]\n",
        "logit_bias = {key: 100 for key in tokenized_labels}\n",
        "# Calculate how many input tokens we can afford for the input prompts (GPT context length is 2049)\n",
        "separator_toks = tokenizer.encode(separator)\n",
        "max_toks = 2049 - len(separator_toks) - 1 # input prompt = max context - separator - completion\n",
        "\n",
        "res_list = []\n",
        "for i in tqdm(range(len(df_test))):\n",
        "    input_prompt = df_test['prompt'][i]\n",
        "    truncated_prompt = tokenizer.decode(tokenizer.encode(input_prompt)[:max_toks]) # Fit into max token length\n",
        "\n",
        "    # Run completion on each output\n",
        "    res = openai.Completion.create(model=ft_model, prompt=truncated_prompt + separator, max_tokens=1, temperature=0, logprobs=2, logit_bias=logit_bias)\n",
        "    res_list.append(res)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZZiX9bwEU74r"
      },
      "outputs": [],
      "source": [
        "# Convert completion results into 0-1 prediction scores\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "scores = []\n",
        "for res in res_list:\n",
        "    # Extract logprobs for the two output classes\n",
        "    choice, top_logprobs = res['choices'][0]['text'], res['choices'][0]['logprobs']['top_logprobs'][0]\n",
        "    logprobs = [top_logprobs[label] for label in [pos_label, neg_label]]\n",
        "    # Convert logprobs into probs\n",
        "    probs = [np.exp(lp) for lp in logprobs]\n",
        "    assert np.isclose(np.sum(probs), 1)\n",
        "    scores.append(probs[0])\n",
        "assert len(scores) == 2771"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LpDqnDm6t9xS",
        "outputId": "ce7a9410-3ad4-46d4-aef6-e59730c68c8d"
      },
      "outputs": [],
      "source": [
        "# Save predictions to file\n",
        "\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "\n",
        "data_dir = Path(\".\")\n",
        "\n",
        "outfile = data_dir / f\"predictions_gpt3_{ft_model}.csv\"\n",
        "assert not outfile.exists(), f\"{outfile} already exists!\"\n",
        "scores = np.array(scores)\n",
        "out_class = scores > 0.5 # Binary classification\n",
        "out_uncertainty = np.minimum(scores, 1 - scores) # Uncertainty score is just how close we are to 0.5\n",
        "pd.DataFrame({\n",
        "    'class': out_class,\n",
        "    'uncertainty': out_uncertainty,\n",
        "}).to_csv(outfile, index=False)\n",
        "print(\"Saved to\", outfile)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "finetune_gpt3.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.8.9 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.9"
    },
    "vscode": {
      "interpreter": {
        "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
